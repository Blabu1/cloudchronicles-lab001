{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1e0979",
   "metadata": {},
   "source": [
    "# üß† cloudChronicles Lab #001: Disaster Recovery Detective\n",
    "\n",
    "**Lab Type:** Idea  \n",
    "**Estimated Time:** 30‚Äì45 mins  \n",
    "**Skill Level:** Beginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by printing your name to personalize the notebook\n",
    "your_name = \"\"\n",
    "print(f\"Welcome to the lab, {your_name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94d019",
   "metadata": {},
   "source": [
    "## üîç STAR Method Lab Prompt\n",
    "\n",
    "**Situation:**  \n",
    "A regional outage has occurred in Google Cloud's us-central1 region, impacting critical applications and data hosted within this region. Services are unavailable, and business operations are severely affected.\n",
    "\n",
    "**Task:**  \n",
    "To restore critical business operations and data accessibility with minimal downtime and data loss, leveraging Google Cloud's disaster recovery capabilities.\n",
    "\n",
    "**Action:**  \n",
    "Our disaster recovery strategy focuses on active-passive failover for critical services, leveraging geographically dispersed resources and automated alerting.

    Pre-computation/Pre-deployment (Before the Outage):

    Cloud SQL:
    High Availability & Cross-Region Replicas: All production Cloud SQL instances in us-central1 have been configured with read replicas in a separate, designated recovery region (e.g., us-east4). These replicas are asynchronous but critical for recovery.
    Automated Backups: Daily automated backups are configured for all Cloud SQL instances, with storage in multi-regional Cloud Storage buckets for resilience.
    Cloud Storage:
    Multi-Region Buckets: All critical data requiring high availability and low latency across regions is stored in multi-region Cloud Storage buckets (e.g., US multi-region). This ensures data is automatically replicated across multiple geographic locations.
    Regional Buckets (for less critical data): Less critical, regional data may reside in regional buckets within us-central1. For this data, we have established clear RTO/RPO expectations and recovery procedures involving restoring from backups to a new region if needed.
    Compute Engine/GKE:
    Application Deployment: Our primary application deployments are in us-central1. However, we have pre-provisioned infrastructure (e.g., GKE clusters, Compute Engine instance templates) in the recovery region (us-east4), ready for rapid deployment.
    Configuration Management: All application configurations, infrastructure-as-code (IaC), and deployment scripts are version-controlled and stored in a highly available repository (e.g., Cloud Source Repositories with multi-region backup).
    Networking:     
    Global Load Balancing: Critical application endpoints are exposed via Global External HTTP(S) Load Balancers. This allows for traffic redirection to healthy backends in other regions.
    DNS: We utilize Cloud DNS with health checks to automatically failover traffic by updating DNS records in case of an outage.
    Pub/Sub:
    Cross-Region Subscriptions: Critical Pub/Sub topics have subscriptions defined in consumers located in both us-central1 and us-east4. This allows consumers in the healthy region to process messages.
    Alerting Integration: Pub/Sub is integrated with Cloud Monitoring and Cloud Logging to trigger alerts on critical system events, including regional outages.
    Monitoring and Alerting:
    Cloud Monitoring: Comprehensive dashboards and alerts are set up to monitor the health and performance of all services across regions.
    Cloud Logging: Centralized logging to aid in post-incident analysis.
    Alerting Channels: Alerts are configured to notify on-call personnel via multiple channels (e.g., PagerDuty, Slack, email). Specifically, alerts are triggered when us-central1 services report widespread unavailability or high error rates.
    During the Outage:
    Phase 1: Detection and Notification:
    Pub/Sub Alerts: Cloud Monitoring, detecting the us-central1 outage (e.g., widespread instance health check failures, high latency, API errors), publishes messages to a dedicated Pub/Sub topic.
    Automated Notification: Subscribers to this Pub/Sub topic (e.g., Cloud Functions, integration with PagerDuty) trigger immediate notifications to the disaster recovery team.
    Manual Verification: The disaster recovery team verifies the scope and impact of the outage through the Google Cloud Status Dashboard and internal monitoring tools.
    Phase 2: Failover Execution:   
    Cloud SQL Failover:
    The primary Cloud SQL instance in us-central1 is unavailable.
    The disaster recovery team initiates the promotion of the Cloud SQL read replica in us-east4 to a standalone primary instance. This is a manual or semi-automated step, as automated failover for cross-region Cloud SQL replicas is not native in all scenarios.
    Application connection strings are updated (either via configuration management or DNS changes) to point to the new primary in us-east4.
    Compute Engine/GKE Application Failover:
    Global External HTTP(S) Load Balancer: Traffic is automatically directed away from the unhealthy us-central1 backends to the pre-provisioned, healthy backends in us-east4. This relies on health checks configured on the load balancer.
    GKE: If GKE is used, a new GKE cluster in us-east4 is rapidly scaled up, and applications are deployed to it using pre-configured IaC scripts. The load balancer is updated to include the new cluster's services as backends.
    Cloud Storage: No specific action is required for data stored in multi-region Cloud Storage buckets, as access automatically shifts to the healthy regional replicas.
    Pub/Sub Message Processing: Consumers in us-east4 continue to pull and process messages from the Pub/Sub topics, ensuring message delivery despite the us-central1 outage.
    DNS Updates: Cloud DNS health checks automatically update DNS records to point to the us-east4 resources. Manual DNS updates may be performed for specific services if automatic health checks are not yet fully configured.
    Phase 3: Service Validation:
    The disaster recovery team thoroughly tests all critical application functionalities in the us-east4 region to ensure full operational capability.
    Monitoring dashboards are closely observed for any anomalies.
    Post-Outage (Recovery and Reversion):
    Phase 1: us-central1 Restoration and Data Synchronization:
    Once us-central1 services are restored by Google Cloud, a new Cloud SQL replica is created in us-central1, pointing to the us-east4 primary.
    Data is allowed to synchronize from us-east4 back to us-central1.
    Phase 2: Planned Reversion (Optional but Recommended): 
    After us-central1 is stable and data is fully synchronized, a planned failback to us-central1 can be executed during a maintenance window. This involves:
    Promoting the us-central1 replica to primary (once it's fully synchronized and healthy).
    Updating application connection strings and load balancer configurations to point back to us-central1.
    Creating a new replica in us-east4 from the us-central1 primary.
    This step ensures that our primary operational region remains us-central1 and that our DR strategy is routinely exercised.
    Phase 3: Post-Mortem and Improvement:
    A thorough post-mortem analysis is conducted to identify root causes, assess the effectiveness of the DR plan, and identify areas for improvement. This includes:
    Reviewing Pub/Sub alert timeliness and accuracy.
    Analyzing Cloud SQL replica promotion time.
    Evaluating application failover performance.
    Updating the disaster recovery plan and runbooks based on lessons learned.
    Regular disaster recovery drills are scheduled to test and refine the plan.\n",
    "\n",
    "**Expected Result:**  \n",
    "By implementing this STAR-based disaster recovery plan, we aim to achieve the following:

    Minimized Downtime: Achieve an RTO (Recovery Time Objective) of under 4 hours for critical applications.
    Minimized Data Loss: Achieve an RPO (Recovery Point Objective) of under 15 minutes for critical data through Cloud SQL replicas and multi-region Cloud Storage.
    Automated Detection and Alerting: Leverage Pub/Sub and Cloud Monitoring for rapid detection and notification of outages, reducing manual intervention time.
    Resilient Data Storage: Ensure continuous data availability through multi-region Cloud Storage, eliminating data loss due to regional outages.
    Streamlined Failover: Facilitate a structured and efficient failover process for critical services to the recovery region.
    Improved Business Continuity: Maintain critical business operations and data accessibility even in the face of a significant regional outage, minimizing financial and reputational impact.
    This plan emphasizes preparation, automation where possible, and clear steps for response and recovery, ensuring business continuity in the event of a Google Cloud regional outage. Regular testing and refinement are crucial for its continued effectiveness.
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b221d",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Your Assignment\n",
    "\n",
    "_Use this section to complete your deliverable:_\n",
    "\n",
    "```markdown\n",
    "(Example Format)\n",
    "\n",
    "- **Primary Region**: us-central1  \n",
    "- **Backup Location**: us-east1  \n",
    "- **Failover Trigger**: Load balancer health check + Pub/Sub alert  \n",
    "- **Redundancy Services**:  \n",
    "   - Cloud SQL with failover  \n",
    "   - Cloud Storage versioning  \n",
    "   - Cloud Functions for health monitoring  \n",
    "- **Backup Schedule**: Every 6 hours, daily export to multi-region bucket  \n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
